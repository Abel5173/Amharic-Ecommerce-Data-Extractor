import re
import os
import pandas as pd
import logging
from transformers import pipeline
import nltk

# -------------------- ðŸ”§ SETUP -------------------- #
nltk.download('punkt', quiet=True)
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# -------------------- ðŸ“¦ PATTERNS -------------------- #
PRICE_PATTERNS = [
    r'á‹‹áŒ‹[:á¡á¢\-]?\s*\d+(?:[,.\d]+)?\s*á‰¥áˆ­?', r'á‰ \s*\d+(?:[,.\d]+)?\s*á‰¥áˆ­', r'\d+\s*á‰¥áˆ­'
]

LOCATION_PATTERNS = [
    r'(?:áŠ á‹²áˆµ\s*áŠ á‰ á‰£|á‰¦áˆŒ|áˆ˜áˆµáŠ¨áˆ¨áˆ|á‰¤á‰°áŠ­áˆ­áˆµá‰²á‹«áŠ•|áˆžáˆ|áˆŠáá‰±|áˆ…áŠ•áƒ|áŒŽáŠ•)',  # Common Amharic locations
    r'á‰ [^\s]+(?:\s+[^\s]+)?',  # á‰ á‰¦áˆŒ áˆŠáá‰±
    r'[^\s]+(?:\s+[^\s]+)?\s*(áŠ áŠ«á‰£á‰¢|áˆ‹á‹­)'  # á‰¦áˆŒ áŠ áŠ«á‰£á‰¢
]

PRODUCT_PATTERNS = [
    r'(GROOMING SET|Steam Iron|Hot plate|Bath brush|glass water set|juicer|bottle)',  # EN
    r'(áˆ›áˆ½áŠ•|áˆ¼á‰¨áˆ­|á‰¶áŠ•á‹¶áˆµ|áˆ˜á‰¶áŠ¨áˆ»|áŒ«áˆ›|á‰€áˆšáˆµ|á‹¨á€áŒ‰áˆ­ á‰¶áŠ•á‹¶áˆµ|áˆá‰¥áˆµ)'  # AM
]

# -------------------- ðŸ¤– NER MODEL -------------------- #
try:
    ner = pipeline("ner", model="Davlan/bert-base-multilingual-cased-ner-hrl", grouped_entities=True)
    logger.info("âœ… HuggingFace NER loaded.")
except Exception as e:
    ner = None
    logger.warning(f"âŒ Could not load NER model: {e}")

# -------------------- âœ‚ï¸ TOKENIZER -------------------- #
def tokenize(text):
    if not isinstance(text, str):
        return []
    return re.findall(r'\b[\u1200-\u137F\w]+\b', text)

# -------------------- ðŸ·ï¸ BIO TAGGING -------------------- #
def bio_tagging(text, tokens):
    tags = ['O'] * len(tokens)

    def mark_entities(tag, match):
        match_tokens = tokenize(match.group())
        for i in range(len(tokens) - len(match_tokens) + 1):
            if tokens[i:i+len(match_tokens)] == match_tokens:
                tags[i] = f"B-{tag}"
                for j in range(1, len(match_tokens)):
                    tags[i + j] = f"I-{tag}"
                break

    # ðŸ§  Rule-based tagging
    for pattern in PRICE_PATTERNS:
        for m in re.finditer(pattern, text): mark_entities("PRICE", m)

    for pattern in LOCATION_PATTERNS:
        for m in re.finditer(pattern, text): mark_entities("LOC", m)

    for pattern in PRODUCT_PATTERNS:
        for m in re.finditer(pattern, text): mark_entities("PRODUCT", m)

    # ðŸ¤– Model-based tagging (only locations)
    if ner:
        try:
            model_results = ner(text)
            for ent in model_results:
                word = ent['word'].replace('##', '')
                for i, tok in enumerate(tokens):
                    if tok.startswith(word) and tags[i] == "O":
                        prefix = "B-LOC" if ent['entity_group'] == "LOC" else "O"
                        tags[i] = prefix
                        break
        except Exception as e:
            logger.warning(f"Model NER error: {e}")
    
    return tags

# -------------------- ðŸ“„ CoNLL Formatting -------------------- #
def format_conll(tokens, tags):
    return "\n".join(f"{tok}\t{tag}" for tok, tag in zip(tokens, tags)) + "\n"

# -------------------- ðŸ“Š Main Processor -------------------- #
def process_conll(csv_path, output_path, limit=50):
    if not os.path.exists(csv_path):
        logger.error(f"CSV not found at {csv_path}")
        return

    df = pd.read_csv(csv_path).fillna('')
    logger.info(f"ðŸ“‚ Loaded {len(df)} messages")

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    count = 0
    conll_lines = []

    for _, row in df.iterrows():
        if count >= limit:
            break
        lang = row.get("Language", "")
        text = row.get("Message Text", row.get("Preprocessed Text", ""))
        if not isinstance(text, str) or lang not in ["am", "en", "mixed"]:
            continue
        tokens = tokenize(text)
        tags = bio_tagging(text, tokens)
        conll_lines.append(format_conll(tokens, tags))
        count += 1
        logger.info(f"Processed: {count}/{limit}")

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("\n".join(conll_lines))
    logger.info(f"âœ… CoNLL data saved: {output_path}")
